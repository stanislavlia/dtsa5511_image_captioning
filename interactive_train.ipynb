{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c492f0bd-ff74-4705-9dcc-bd09634aa0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 16:24:50.638601: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-11 16:24:50.661213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras import layers\n",
    "\n",
    "from train_config import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762f9d9c-b04f-410d-99e7-121a209a19c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100009...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;START&gt;  Two young guys with shaggy hair look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100009...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;START&gt;  Two young , White males are outside n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100009...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;START&gt;  Two men in green shirts are standing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100009...</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;START&gt;  A man in a blue shirt standing in a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100009...</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;START&gt;  Two friends enjoy time spent together...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100024...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;START&gt;  Several men in hard hats are operatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100024...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;START&gt;  Workers look down from up above on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100024...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;START&gt;  Two men working on a machine wearing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100024...</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;START&gt;  Four men on top of a tall structure ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/flickr30k_images/flickr30k_images//100024...</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;START&gt;  Three men on a large rig . &lt;END&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name comment_number  \\\n",
       "0  data/flickr30k_images/flickr30k_images//100009...              0   \n",
       "1  data/flickr30k_images/flickr30k_images//100009...              1   \n",
       "2  data/flickr30k_images/flickr30k_images//100009...              2   \n",
       "3  data/flickr30k_images/flickr30k_images//100009...              3   \n",
       "4  data/flickr30k_images/flickr30k_images//100009...              4   \n",
       "5  data/flickr30k_images/flickr30k_images//100024...              0   \n",
       "6  data/flickr30k_images/flickr30k_images//100024...              1   \n",
       "7  data/flickr30k_images/flickr30k_images//100024...              2   \n",
       "8  data/flickr30k_images/flickr30k_images//100024...              3   \n",
       "9  data/flickr30k_images/flickr30k_images//100024...              4   \n",
       "\n",
       "                                             comment  \n",
       "0  <START>  Two young guys with shaggy hair look ...  \n",
       "1  <START>  Two young , White males are outside n...  \n",
       "2  <START>  Two men in green shirts are standing ...  \n",
       "3  <START>  A man in a blue shirt standing in a g...  \n",
       "4  <START>  Two friends enjoy time spent together...  \n",
       "5  <START>  Several men in hard hats are operatin...  \n",
       "6  <START>  Workers look down from up above on a ...  \n",
       "7  <START>  Two men working on a machine wearing ...  \n",
       "8  <START>  Four men on top of a tall structure ....  \n",
       "9          <START>  Three men on a large rig . <END>  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captionings_df = pd.read_csv(os.path.join(DATA_PATH, \"results.csv\"), sep=\"|\").dropna()\n",
    "captionings_df.columns = [\"image_name\", \"comment_number\", \"comment\"]\n",
    "captionings_df[\"image_name\"] = IMAGES_PATH + \"/\" + captionings_df[\"image_name\"] \n",
    "\n",
    "\n",
    "#ADDING START AND END special tokens\n",
    "captionings_df[\"comment\"] = \"<START> \" + captionings_df[\"comment\"] + \" <END>\"\n",
    "captionings_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b4e0f7-db62-47e4-a62d-1c3697133cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image-text examples:  150968\n",
      "Validation image-text examples:  7946\n"
     ]
    }
   ],
   "source": [
    "#Shuffle df\n",
    "captionings_df = captionings_df.sample(frac=1,\n",
    "                                       random_state=42,\n",
    "                                       replace=False,\n",
    "                                       )\n",
    "\n",
    "\n",
    "n_train_examples = int(len(captionings_df) * (1 - VAL_FRACTION))\n",
    "\n",
    "train_captionings_df = captionings_df[ : n_train_examples]\n",
    "val_captionings_df = captionings_df[n_train_examples : ]\n",
    "\n",
    "print(\"Train image-text examples: \", train_captionings_df.shape[0])\n",
    "print(\"Validation image-text examples: \", val_captionings_df.shape[0])\n",
    "\n",
    "#save splits\n",
    "train_captionings_df.to_csv(\"splits/train_captions.csv\", index=False)\n",
    "val_captionings_df.to_csv(\"splits/val_captions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2c3495-4266-4dde-ab22-5a68224c6a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 16:24:53.983358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.002965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.003053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.005301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.005368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.005411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.059693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.059771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.059816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 16:24:54.059863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1244 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from data_processing import build_tokenizer, build_image_augmenter,  decode_and_resize\n",
    "\n",
    "\n",
    "tokenizer = build_tokenizer()\n",
    "tokenizer.adapt(train_captionings_df[\"comment\"].tolist())\n",
    "\n",
    "def process_input(img_path, captions):\n",
    "    return decode_and_resize(img_path), tf.reshape(tokenizer(captions), shape=(1, SEQ_LENGTH))\n",
    "\n",
    "def make_dataset(images, captions):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
    "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
    "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af62a91a-b351-4264-869b-76b8c3aeb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_captionings_df[\"image_name\"].tolist(),\n",
    "                             train_captionings_df[\"comment\"].tolist())\n",
    "\n",
    "val_dataset = make_dataset(train_captionings_df[\"image_name\"].tolist(),\n",
    "                             train_captionings_df[\"comment\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fff3d9-965a-4287-b1c9-7709e54e38f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 224, 224, 3) (64, 1, 32)\n",
      "(64, 224, 224, 3) (64, 1, 32)\n",
      "(64, 224, 224, 3) (64, 1, 32)\n",
      "(64, 224, 224, 3) (64, 1, 32)\n",
      "(64, 224, 224, 3) (64, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "for img, cap in val_dataset.take(5):\n",
    "    print(img.shape, cap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d75852-aa1d-46ca-997a-a32a7580877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerDecoderBlock, TransformerEncoderBlock, ImageCaptioningModel, get_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6385b918-d10f-4abc-a36c-0a5ae146c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.efficientnet.EfficientNetB1(\n",
    "        input_shape=(*IMAGE_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "\n",
    "cnn = get_cnn_model(base_model)\n",
    "\n",
    "encoder = TransformerEncoderBlock(\n",
    "    embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=ENC_HEADS\n",
    ")\n",
    "decoder = TransformerDecoderBlock(\n",
    "    embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=DEC_HEADS, \n",
    ")\n",
    "\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn,\n",
    "    #image_aug=None,\n",
    "    encoder=encoder, \n",
    "    decoder=decoder\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3fce7b0-681a-4904-b83f-8935c986e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iter(train_dataset.take(1))) #SANITY CHECK\n",
    "\n",
    "\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(0.01), loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea80769-870b-4bd4-8618-592301934bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 16:25:02.214783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-04-11 16:25:02.224340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2024-04-11 16:25:02.410251: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x75e0796c9950 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-11 16:25:02.410275: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090 Laptop GPU, Compute Capability 8.9\n",
      "2024-04-11 16:25:02.412853: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-11 16:25:02.474638: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 96ms/step - loss: 8.2319 - acc: 0.0670\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 7.4057 - acc: 0.1029\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 7.5297 - acc: 0.0485\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.4118 - acc: 0.0113\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 5.6087 - acc: 0.0655\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 5.6989 - acc: 0.1061\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 5.9244 - acc: 0.1147\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 5.5853 - acc: 0.0898\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.3817 - acc: 0.0821\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.2753 - acc: 0.1188\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 5.2317 - acc: 0.1150\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 5.1318 - acc: 0.1016\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.1244 - acc: 0.1079\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 5.0646 - acc: 0.1110\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.0478 - acc: 0.1176\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.0330 - acc: 0.1044\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.0029 - acc: 0.1146\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 4.9989 - acc: 0.1160\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 4.9656 - acc: 0.1237\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.9354 - acc: 0.1197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x75e41dbb8790>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_model.fit(X_batch, y_batch, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b411030-7c0c-48e1-9538-acdd5ff2dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "caption_model.save_weights(\"caption_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e046ff3-49a2-48f5-9686-5146b7b48c48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"image_captioning_model_1\" (type ImageCaptioningModel).\n\nin user code:\n\n    File \"/home/user/Desktop/image_captioning/model.py\", line 178, in call  *\n        x = self.decoder(inputs[2],x,training=inputs[1],mask=None)\n    File \"/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filejyh3zvi3.py\", line 32, in tf__call\n        attention_output_1 = ag__.converted_call(ag__.ld(self).attention_1, (), dict(query=ag__.ld(inputs), value=ag__.ld(inputs), key=ag__.ld(inputs), attention_mask=ag__.ld(combined_mask), training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer 'transformer_decoder_block_1' (type TransformerDecoderBlock).\n    \n    in user code:\n    \n        File \"/home/user/Desktop/image_captioning/model.py\", line 117, in call  *\n            attention_output_1 = self.attention_1(\n        File \"/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'softmax_1' (type Softmax).\n        \n        Dimensions must be equal, but are 1280 and 49 for '{{node image_captioning_model_1/transformer_decoder_block_1/multi_head_attention_4/softmax_1/add}} = AddV2[T=DT_FLOAT](image_captioning_model_1/transformer_decoder_block_1/multi_head_attention_4/einsum/Einsum, image_captioning_model_1/transformer_decoder_block_1/multi_head_attention_4/softmax_1/mul)' with input shapes: [?,2,49,1280,49,1280], [1,1,?,49,49,1280].\n        \n        Call arguments received by layer 'softmax_1' (type Softmax):\n          • inputs=tf.Tensor(shape=(None, 2, 49, 1280, 49, 1280), dtype=float32)\n          • mask=tf.Tensor(shape=(1, 1, None, 49, 49, 1280), dtype=bool)\n    \n    \n    Call arguments received by layer 'transformer_decoder_block_1' (type TransformerDecoderBlock):\n      • inputs=tf.Tensor(shape=(None, 49, 1280), dtype=float32)\n      • encoder_outputs=tf.Tensor(shape=(None, 49, 512), dtype=float32)\n      • training=tf.Tensor(shape=(), dtype=bool)\n      • mask=None\n\n\nCall arguments received by layer \"image_captioning_model_1\" (type ImageCaptioningModel):\n  • inputs=['tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(), dtype=bool)', 'tf.Tensor(shape=(None, 49, 1280), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129877/1580299831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trained_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"caption_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/image_captioning/training_utils.py\u001b[0m in \u001b[0;36mload_trained_model_weights\u001b[0;34m(path_to_weights)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mcaption_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn_input\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#loading weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/image_captioning/model.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/image_captioning/model.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, encoder_outputs, training, mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'combined_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'padding_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mattention_output_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mout_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mattention_output_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"image_captioning_model_1\" (type ImageCaptioningModel).\n\nin user code:\n\n    File \"/home/user/Desktop/image_captioning/model.py\", line 178, in call  *\n        x = self.decoder(inputs[2],x,training=inputs[1],mask=None)\n    File \"/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filejyh3zvi3.py\", line 32, in tf__call\n        attention_output_1 = ag__.converted_call(ag__.ld(self).attention_1, (), dict(query=ag__.ld(inputs), value=ag__.ld(inputs), key=ag__.ld(inputs), attention_mask=ag__.ld(combined_mask), training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer 'transformer_decoder_block_1' (type TransformerDecoderBlock).\n    \n    in user code:\n    \n        File \"/home/user/Desktop/image_captioning/model.py\", line 117, in call  *\n            attention_output_1 = self.attention_1(\n        File \"/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer 'softmax_1' (type Softmax).\n        \n        Dimensions must be equal, but are 1280 and 49 for '{{node image_captioning_model_1/transformer_decoder_block_1/multi_head_attention_4/softmax_1/add}} = AddV2[T=DT_FLOAT](image_captioning_model_1/transformer_decoder_block_1/multi_head_attention_4/einsum/Einsum, image_captioning_model_1/transformer_decoder_block_1/multi_head_attention_4/softmax_1/mul)' with input shapes: [?,2,49,1280,49,1280], [1,1,?,49,49,1280].\n        \n        Call arguments received by layer 'softmax_1' (type Softmax):\n          • inputs=tf.Tensor(shape=(None, 2, 49, 1280, 49, 1280), dtype=float32)\n          • mask=tf.Tensor(shape=(1, 1, None, 49, 49, 1280), dtype=bool)\n    \n    \n    Call arguments received by layer 'transformer_decoder_block_1' (type TransformerDecoderBlock):\n      • inputs=tf.Tensor(shape=(None, 49, 1280), dtype=float32)\n      • encoder_outputs=tf.Tensor(shape=(None, 49, 512), dtype=float32)\n      • training=tf.Tensor(shape=(), dtype=bool)\n      • mask=None\n\n\nCall arguments received by layer \"image_captioning_model_1\" (type ImageCaptioningModel):\n  • inputs=['tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(), dtype=bool)', 'tf.Tensor(shape=(None, 49, 1280), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from training_utils import load_trained_model_weights\n",
    "\n",
    "\n",
    "\n",
    "new_model = load_trained_model_weights(\"caption_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535f85ce-115e-4529-b5f6-8ead5198af5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"image_captioning_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_1 (Functional)        (None, 49, 1280)          6575239   \n",
      "                                                                 \n",
      " transformer_encoder_block_  multiple                  1710080   \n",
      " 1 (TransformerEncoderBlock                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " transformer_decoder_block_  multiple                  14733840  \n",
      " 1 (TransformerDecoderBlock                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23019163 (87.81 MB)\n",
      "Trainable params: 16443920 (62.73 MB)\n",
      "Non-trainable params: 6575243 (25.08 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bbf716-bee6-4b7e-b9b3-d7a01d6e3f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
